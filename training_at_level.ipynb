{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "training-at-level.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vaguiar/camelyon_dl_2019/blob/data-exploration/training_at_level.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntvJ9R35tjZn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 700
        },
        "outputId": "54240a0a-8841-4aec-c2a5-88174fdee2cb"
      },
      "source": [
        "!pip  install tf-nightly-2.0-preview"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tf-nightly-2.0-preview\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a5/ed/82ce19adbd413ae613212330b18aedf18be4535ccdebf50bf2380e7bdfb4/tf_nightly_2.0_preview-2.0.0.dev20190511-cp36-cp36m-manylinux1_x86_64.whl (87.5MB)\n",
            "\u001b[K     |████████████████████████████████| 87.5MB 262kB/s \n",
            "\u001b[?25hCollecting tensorflow-estimator-2.0-preview (from tf-nightly-2.0-preview)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/2b/c359cebd1c3d3eeec25f3d54d30e11e074c4bef3377cf51ddb3b44df3e08/tensorflow_estimator_2.0_preview-1.14.0.dev2019051100-py2.py3-none-any.whl (427kB)\n",
            "\u001b[K     |████████████████████████████████| 430kB 35.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.0.9)\n",
            "Collecting wrapt>=1.11.1 (from tf-nightly-2.0-preview)\n",
            "  Downloading https://files.pythonhosted.org/packages/67/b2/0f71ca90b0ade7fad27e3d20327c996c6252a2ffe88f50a95bba7434eda9/wrapt-1.11.1.tar.gz\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (0.7.1)\n",
            "Collecting google-pasta>=0.1.6 (from tf-nightly-2.0-preview)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f9/68/a14620bfb042691f532dcde8576ff82ee82e4c003cdc0a3dbee5f289cee6/google_pasta-0.1.6-py3-none-any.whl (51kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 15.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.12.0)\n",
            "Collecting tb-nightly<1.15.0a0,>=1.14.0a0 (from tf-nightly-2.0-preview)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ca/84/bc3cd590124ab4adac1f46c5166cffe73c1b9b6b51300d3d57cd7e5b27e0/tb_nightly-1.14.0a20190511-py3-none-any.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1MB 35.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (0.33.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.0.7)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.16.3)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (3.7.1)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (0.2.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.1.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.15.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.15.0a0,>=1.14.0a0->tf-nightly-2.0-preview) (0.15.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.15.0a0,>=1.14.0a0->tf-nightly-2.0-preview) (3.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tf-nightly-2.0-preview) (2.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tf-nightly-2.0-preview) (41.0.1)\n",
            "Building wheels for collected packages: wrapt\n",
            "  Building wheel for wrapt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/89/67/41/63cbf0f6ac0a6156588b9587be4db5565f8c6d8ccef98202fc\n",
            "Successfully built wrapt\n",
            "\u001b[31mERROR: thinc 6.12.1 has requirement wrapt<1.11.0,>=1.10.0, but you'll have wrapt 1.11.1 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorflow-estimator-2.0-preview, wrapt, google-pasta, tb-nightly, tf-nightly-2.0-preview\n",
            "  Found existing installation: wrapt 1.10.11\n",
            "    Uninstalling wrapt-1.10.11:\n",
            "      Successfully uninstalled wrapt-1.10.11\n",
            "Successfully installed google-pasta-0.1.6 tb-nightly-1.14.0a20190511 tensorflow-estimator-2.0-preview-1.14.0.dev2019051100 tf-nightly-2.0-preview-2.0.0.dev20190511 wrapt-1.11.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08XPVYEYtof_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fbf967ef-8005-46bd-e450-6f2f9f97e2e1"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# The code in this notebook should work identically in TF v1 and v2\n",
        "import tensorflow as tf\n",
        "import zipfile\n",
        "\n",
        "import tensorflow.keras.applications as applications\n",
        "from tensorflow.keras.layers import Conv2D, Dense, Dropout, Flatten, MaxPooling2D\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "print(\"You have version\", tf.__version__)\n",
        "assert tf.__version__ >= \"2.0\" # TensorFlow ≥ 2.0 required"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "You have version 2.0.0-dev20190511\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YW8_045QucD4",
        "colab_type": "text"
      },
      "source": [
        "### Global constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ruz9r2UjubX5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Images will be resized to(TARGET_SHAPE, TARGET_SHAPE) as they're read off disk.\n",
        "TARGET_SHAPE = 150 \n",
        "BATCH_SIZE = 25\n",
        "\n",
        "CLASS_MODE = 'categorical'\n",
        "\n",
        "NUM_OF_CLASSES = 2\n",
        "CLASS_NAMES = ['tumor', 'non_tumor']\n",
        "\n",
        "#################\n",
        "EPOCHS = 50"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydSekjDWXHMY",
        "colab_type": "text"
      },
      "source": [
        "### Setting up GCP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8CacwAVXBSW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZrOGONKRvCZw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "GCP_PROJECT_ID = 'triple-voyage-239123'\n",
        "GCP_BUCKET_NAME = 'vaa2114_dl_2019'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8I33849Eqdy",
        "colab_type": "code",
        "outputId": "6c512bc3-e58f-4a7e-b959-2b5444182ba2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!gcloud config set project {GCP_PROJECT_ID}"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Updated property [core/project].\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N45WU3AFvIC_",
        "colab_type": "text"
      },
      "source": [
        "### Read Data from GCP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gewIRyRPvlk7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TRAIN_DIR = \"/tmp/train/\"\n",
        "GCP_TRAIN_DIR = \"train/level5/\"\n",
        "\n",
        "!mkdir -p '$TRAIN_DIR'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_ToRfu3yNWI",
        "colab_type": "text"
      },
      "source": [
        "### Download Level 5 data into Training Dir"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0LVeuwBvMdK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        },
        "outputId": "af025f45-2956-4222-8933-a978276214a3"
      },
      "source": [
        "# Download the file.\n",
        "!gsutil -m cp -r gs://{GCP_BUCKET_NAME}/{GCP_TRAIN_DIR} {TRAIN_DIR}"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying gs://vaa2114_dl_2019/train/level5/non_tumor/level_5_0_108800_tumor_078.tif...\n",
            "Copying gs://vaa2114_dl_2019/train/level5/non_tumor/level_5_0_217600_tumor_035.tif...\n",
            "Copying gs://vaa2114_dl_2019/train/level5/non_tumor/level_5_0_217600_tumor_057.tif...\n",
            "Copying gs://vaa2114_dl_2019/train/level5/non_tumor/level_5_0_96000_tumor_094.tif...\n",
            "- [4 files][143.2 KiB/143.2 KiB]                                                \n",
            "==> NOTE: You are performing a sequence of gsutil operations that may\n",
            "run significantly faster if you instead use gsutil -m cp ... Please\n",
            "see the -m section under \"gsutil help options\" for further information\n",
            "about when gsutil -m can be advantageous.\n",
            "\n",
            "Copying gs://vaa2114_dl_2019/train/level5/non_tumor/level_5_102400_12800_tumor_094.tif...\n",
            "Copying gs://vaa2114_dl_2019/train/level5/non_tumor/level_5_102400_19200_tumor_094.tif...\n",
            "Copying gs://vaa2114_dl_2019/train/level5/non_tumor/level_5_102400_19200_tumor_101.tif...\n",
            "Copying gs://vaa2114_dl_2019/train/level5/non_tumor/level_5_102400_25600_tumor_094.tif...\n",
            "Copying gs://vaa2114_dl_2019/train/level5/non_tumor/level_5_102400_25600_tumor_096.tif...\n",
            "Copying gs://vaa2114_dl_2019/train/level5/non_tumor/level_5_102400_25600_tumor_101.tif...\n",
            "Copying gs://vaa2114_dl_2019/train/level5/non_tumor/level_5_102400_32000_tumor_096.tif...\n",
            "Copying gs://vaa2114_dl_2019/train/level5/non_tumor/level_5_102400_32000_tumor_101.tif...\n",
            "Copying gs://vaa2114_dl_2019/train/level5/non_tumor/level_5_102400_38400_tumor_094.tif...\n",
            "Copying gs://vaa2114_dl_2019/train/level5/non_tumor/level_5_102400_38400_tumor_096.tif...\n",
            "Copying gs://vaa2114_dl_2019/train/level5/non_tumor/level_5_102400_38400_tumor_101.tif...\n",
            "Copying gs://vaa2114_dl_2019/train/level5/non_tumor/level_5_102400_44800_tumor_096.tif...\n",
            "Copying gs://vaa2114_dl_2019/train/level5/non_tumor/level_5_102400_57600_tumor_101.tif...\n",
            "Copying gs://vaa2114_dl_2019/train/level5/non_tumor/level_5_108800_25600_tumor_096.tif...\n",
            "Copying gs://vaa2114_dl_2019/train/level5/non_tumor/level_5_108800_32000_tumor_096.tif...\n",
            "Copying gs://vaa2114_dl_2019/train/level5/non_tumor/level_5_108800_32000_tumor_101.tif...\n",
            "Copying gs://vaa2114_dl_2019/train/level5/non_tumor/level_5_108800_38400_tumor_096.tif...\n",
            "Copying gs://vaa2114_dl_2019/train/level5/non_tumor/level_5_108800_44800_tumor_096.tif...\n",
            "Copying gs://vaa2114_dl_2019/train/level5/non_tumor/level_5_108800_51200_tumor_101.tif...\n",
            "Copying gs://vaa2114_dl_2019/train/level5/non_tumor/level_5_108800_57600_tumor_101.tif...\n",
            "Copying gs://vaa2114_dl_2019/train/level5/non_tumor/level_5_115200_25600_tumor_096.tif...\n",
            "Copying gs://vaa2114_dl_2019/train/level5/non_tumor/level_5_115200_32000_tumor_096.tif...\n",
            "Caught CTRL-C (signal 2) - exiting\n",
            "^C\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYTQJgSbuDZX",
        "colab_type": "text"
      },
      "source": [
        "### Util functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7tLFPD1uCEN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getModel(conv_base):\n",
        "  model = Sequential()\n",
        "  model.add(conv_base)\n",
        "  model.add(Flatten())\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Dense(216, activation='relu'))\n",
        "  model.add(Dense(NUM_OF_CLASSES, activation='softmax'))\n",
        "  \n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-ue8-Eturr3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compile and build\n",
        "\n",
        "def trainModel(model, total_train, total_val, \n",
        "               train_data_gen, val_data_gen, EPOCHS):\n",
        "  model.compile(loss='categorical_crossentropy',\n",
        "                optimizer='adam',\n",
        "                metrics=['acc'])\n",
        "\n",
        "  history = model.fit_generator(\n",
        "        train_data_gen,\n",
        "        steps_per_epoch=int(np.ceil(total_train / float(BATCH_SIZE))),\n",
        "        epochs=EPOCHS,\n",
        "        validation_data=val_data_gen,\n",
        "        validation_steps=int(np.ceil(total_val / float(BATCH_SIZE))),\n",
        "        verbose=2)\n",
        "  \n",
        "  return history"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}